{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd3c67b",
   "metadata": {},
   "source": [
    "# Image Duplicate Detection using MinHash and LSH\n",
    "\n",
    "\n",
    "### Objectives:\n",
    "1. Convert images to feature vectors (using perceptual hashing)\n",
    "2. Apply **MinHashing** and **Locality Sensitive Hashing (LSH)**\n",
    "3. Test on a dataset (≥1000 images)\n",
    "4. Compare results with traditional similarity metrics (e.g., SSIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4d6588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"images\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Parameters\n",
    "HASH_SIZE = 8          # for perceptual hashing (8 → 64 bits)\n",
    "NUM_MINHASH = 100      # number of minhash functions\n",
    "NUM_BANDS = 20         # number of LSH bands (NUM_MINHASH should be divisible by this)\n",
    "HAMMING_THRESHOLD = 5  # threshold for considering duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32367634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perceptual hashes: 100%|██████████| 1500/1500 [00:17<00:00, 84.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hashes saved to c:\\Users\\Ghita\\Documents\\FALL25\\CSC 4352\\ImageDuplicatesDetection\\Project1\\results\\hashes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_phash(image_path, hash_size=HASH_SIZE):\n",
    "    \"\"\"Compute perceptual hash for one image.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\")\n",
    "        ph = imagehash.phash(img, hash_size=hash_size)\n",
    "        return int(str(ph), 16)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "hashes_csv = RESULTS_DIR / \"hashes.csv\"\n",
    "\n",
    "if not hashes_csv.exists():\n",
    "    images = sorted([p for p in DATA_DIR.iterdir() if p.suffix.lower() in ('.jpg', '.png', '.jpeg')])\n",
    "    with open(hashes_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"phash\"])\n",
    "        for p in tqdm(images, desc=\"Computing perceptual hashes\"):\n",
    "            h = compute_phash(p)\n",
    "            if h is not None:\n",
    "                writer.writerow([p.name, h])\n",
    "    print(f\" Hashes saved to {hashes_csv}\")\n",
    "else:\n",
    "    print(\"hashes.csv already exists — skipping computation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e297ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1500 images and built feature sets.\n"
     ]
    }
   ],
   "source": [
    "def int_to_bitset(phash_int, n_bits=64):\n",
    "    \"\"\"Convert integer pHash to a set of bit positions where bit=1.\"\"\"\n",
    "    return {i for i in range(n_bits) if ((phash_int >> i) & 1)}\n",
    "\n",
    "# Read hashes.csv\n",
    "rows = []\n",
    "with open(hashes_csv) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for r in reader:\n",
    "        rows.append((r[\"filename\"], int(r[\"phash\"])))\n",
    "\n",
    "filenames = [r[0] for r in rows]\n",
    "phashes = [r[1] for r in rows]\n",
    "bitsets = [int_to_bitset(h) for h in phashes]\n",
    "\n",
    "print(f\"Loaded {len(filenames)} images and built feature sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f15068b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing MinHash signatures: 100%|██████████| 1500/1500 [00:00<00:00, 1982.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MinHash signatures created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate MinHash functions\n",
    "def create_hash_funcs(num_hashes=NUM_MINHASH, p=(2**31 - 1)):\n",
    "    funcs = []\n",
    "    for _ in range(num_hashes):\n",
    "        a = random.randint(1, p - 1)\n",
    "        b = random.randint(0, p - 1)\n",
    "        funcs.append((a, b, p))\n",
    "    return funcs\n",
    "\n",
    "\n",
    "def minhash_signature(feature_set, hash_funcs):\n",
    "    sig = []\n",
    "    for a, b, p in hash_funcs:\n",
    "        sig.append(min(((a * x + b) % p) for x in feature_set))\n",
    "    return sig\n",
    "\n",
    "\n",
    "hash_funcs = create_hash_funcs(NUM_MINHASH)\n",
    "signatures = [minhash_signature(s, hash_funcs) for s in tqdm(bitsets, desc=\"Computing MinHash signatures\")]\n",
    "\n",
    "print(\" MinHash signatures created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5510aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 158914 candidate pairs.\n"
     ]
    }
   ],
   "source": [
    "class LSHIndex:\n",
    "    def __init__(self, num_bands=NUM_BANDS, rows_per_band=None):\n",
    "        if rows_per_band is None:\n",
    "            if NUM_MINHASH % num_bands != 0:\n",
    "                raise ValueError(\"NUM_MINHASH must be divisible by num_bands\")\n",
    "            rows_per_band = NUM_MINHASH // num_bands\n",
    "        self.num_bands = num_bands\n",
    "        self.rows_per_band = rows_per_band\n",
    "        self.tables = [defaultdict(list) for _ in range(num_bands)]\n",
    "\n",
    "    def _band_hash(self, band):\n",
    "        return hash(tuple(band))\n",
    "\n",
    "    def index(self, doc_id, signature):\n",
    "        for b in range(self.num_bands):\n",
    "            start = b * self.rows_per_band\n",
    "            band = signature[start:start + self.rows_per_band]\n",
    "            key = self._band_hash(band)\n",
    "            self.tables[b][key].append(doc_id)\n",
    "\n",
    "    def get_candidates(self):\n",
    "        candidates = set()\n",
    "        for table in self.tables:\n",
    "            for bucket in table.values():\n",
    "                if len(bucket) > 1:\n",
    "                    for i in range(len(bucket)):\n",
    "                        for j in range(i + 1, len(bucket)):\n",
    "                            candidates.add(tuple(sorted((bucket[i], bucket[j]))))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "lsh = LSHIndex(num_bands=NUM_BANDS)\n",
    "for idx, sig in enumerate(signatures):\n",
    "    lsh.index(idx, sig)\n",
    "\n",
    "candidates = list(lsh.get_candidates())\n",
    "print(f\"Found {len(candidates)} candidate pairs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0463e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verified 949 duplicate pairs below threshold 5.\n",
      "Results saved to c:\\Users\\Ghita\\Documents\\FALL25\\CSC 4352\\ImageDuplicatesDetection\\Project1\\results\\verified_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "def hamming(a, b):\n",
    "    return bin(a ^ b).count(\"1\")\n",
    "\n",
    "\n",
    "verified = []\n",
    "for i, j in candidates:\n",
    "    fn1, fn2 = filenames[i], filenames[j]\n",
    "    ph1, ph2 = phashes[i], phashes[j]\n",
    "    dist = hamming(ph1, ph2)\n",
    "    if dist <= HAMMING_THRESHOLD:\n",
    "        verified.append((fn1, fn2, dist))\n",
    "\n",
    "print(f\" Verified {len(verified)} duplicate pairs below threshold {HAMMING_THRESHOLD}.\")\n",
    "\n",
    "# Save results\n",
    "verified_csv = RESULTS_DIR / \"verified_pairs.csv\"\n",
    "with open(verified_csv, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"image1\", \"image2\", \"hamming_distance\"])\n",
    "    writer.writerows(verified)\n",
    "\n",
    "print(f\"Results saved to {verified_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67c02284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 523 duplicate groups:\n",
      " Total images involved in duplicates: 1218\n",
      "\n",
      " Group size distribution:\n",
      "   Groups of 4 images: 82 groups (328 total images)\n",
      "   Groups of 3 images: 8 groups (24 total images)\n",
      "   Groups of 2 images: 433 groups (866 total images)\n",
      "\n",
      " Top 10 largest duplicate groups:\n",
      "   Group 1 (4 images): image1001.jpg, image1456.jpg, image1495.jpg, image756.jpg\n",
      "   Group 2 (4 images): image1008.jpg, image350.jpg, image61.jpg, image766.jpg\n",
      "   Group 3 (4 images): image101.jpg, image216.jpg, image483.jpg, image704.jpg\n",
      "   Group 4 (4 images): image1010.jpg, image219.jpg, image463.jpg, image664.jpg\n",
      "   Group 5 (4 images): image1024.jpg, image377.jpg, image528.jpg, image898.jpg\n",
      "   Group 6 (4 images): image1027.jpg, image1434.jpg, image360.jpg, image404.jpg\n",
      "   Group 7 (4 images): image1029.jpg, image1395.jpg, image160.jpg, image847.jpg\n",
      "   Group 8 (4 images): image103.jpg, image1292.jpg, image1455.jpg, image425.jpg\n",
      "   Group 9 (4 images): image1032.jpg, image1152.jpg, image25.jpg, image976.jpg\n",
      "   Group 10 (4 images): image1035.jpg, image456.jpg, image731.jpg, image816.jpg\n"
     ]
    }
   ],
   "source": [
    "def group_duplicates(verified_pairs):\n",
    "    \"\"\"\n",
    "    Group duplicate images using Union-Find algorithm.\n",
    "    Input: List of (img1, img2, distance) tuples\n",
    "    Output: List of groups, where each group contains all images that are duplicates of each other\n",
    "    \"\"\"\n",
    "    # Create a mapping from filename to index for easier processing\n",
    "    all_images = set()\n",
    "    for img1, img2, _ in verified_pairs:\n",
    "        all_images.add(img1)\n",
    "        all_images.add(img2)\n",
    "    \n",
    "    image_to_idx = {img: idx for idx, img in enumerate(sorted(all_images))}\n",
    "    idx_to_image = {idx: img for img, idx in image_to_idx.items()}\n",
    "    \n",
    "    # Union-Find data structure\n",
    "    parent = list(range(len(all_images)))\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])  # Path compression\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        root_x, root_y = find(x), find(y)\n",
    "        if root_x != root_y:\n",
    "            parent[root_y] = root_x\n",
    "    \n",
    "    # Union all duplicate pairs\n",
    "    for img1, img2, _ in verified_pairs:\n",
    "        idx1, idx2 = image_to_idx[img1], image_to_idx[img2]\n",
    "        union(idx1, idx2)\n",
    "    \n",
    "    # Group images by their root parent\n",
    "    groups = defaultdict(list)\n",
    "    for idx in range(len(all_images)):\n",
    "        root = find(idx)\n",
    "        groups[root].append(idx_to_image[idx])\n",
    "    \n",
    "    # Return only groups with more than 1 image, sorted by group size\n",
    "    duplicate_groups = [sorted(group) for group in groups.values() if len(group) > 1]\n",
    "    duplicate_groups.sort(key=len, reverse=True)  # Largest groups first\n",
    "    \n",
    "    return duplicate_groups\n",
    "\n",
    "# Group all duplicates\n",
    "duplicate_groups = group_duplicates(verified)\n",
    "\n",
    "print(f\" Found {len(duplicate_groups)} duplicate groups:\")\n",
    "print(f\" Total images involved in duplicates: {sum(len(group) for group in duplicate_groups)}\")\n",
    "\n",
    "# Display group statistics\n",
    "group_sizes = defaultdict(int)\n",
    "for group in duplicate_groups:\n",
    "    group_sizes[len(group)] += 1\n",
    "\n",
    "print(f\"\\n Group size distribution:\")\n",
    "for size in sorted(group_sizes.keys(), reverse=True):\n",
    "    count = group_sizes[size]\n",
    "    total_images = size * count\n",
    "    print(f\"   Groups of {size} images: {count} groups ({total_images} total images)\")\n",
    "\n",
    "# Show the largest groups\n",
    "print(f\"\\n Top 10 largest duplicate groups:\")\n",
    "for i, group in enumerate(duplicate_groups[:10], 1):\n",
    "    print(f\"   Group {i} ({len(group)} images): {', '.join(group[:5])}{'...' if len(group) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0258ec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Duplicate groups saved to: c:\\Users\\Ghita\\Documents\\FALL25\\CSC 4352\\ImageDuplicatesDetection\\Project1\\results\\duplicate_groups.csv\n",
      " Detailed pairs with groups saved to: c:\\Users\\Ghita\\Documents\\FALL25\\CSC 4352\\ImageDuplicatesDetection\\Project1\\results\\detailed_duplicate_pairs.csv\n",
      "\n",
      " Dataset Summary:\n",
      "   Total images in dataset: 1500\n",
      "   Images with duplicates: 1218\n",
      "   Unique images (no duplicates): 805\n",
      "   Duplicate rate: 81.20%\n"
     ]
    }
   ],
   "source": [
    "# Save grouped results to CSV\n",
    "grouped_csv = RESULTS_DIR / \"duplicate_groups.csv\"\n",
    "\n",
    "try:\n",
    "    with open(grouped_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"group_id\", \"group_size\", \"images\"])\n",
    "        \n",
    "        for i, group in enumerate(duplicate_groups, 1):\n",
    "            writer.writerow([f\"Group_{i}\", len(group), \"; \".join(group)])\n",
    "    print(f\" Duplicate groups saved to: {grouped_csv}\")\n",
    "except PermissionError:\n",
    "    print(f\" Error: Cannot write to {grouped_csv}\")\n",
    "    print(\"   The file is currently open in another program (Excel, text editor, etc.)\")\n",
    "    print(\"   Please close the file and run this cell again.\")\n",
    "\n",
    "# Also create a detailed pairs file that shows the relationships\n",
    "detailed_csv = RESULTS_DIR / \"detailed_duplicate_pairs.csv\"\n",
    "\n",
    "try:\n",
    "    with open(detailed_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"group_id\", \"image1\", \"image2\", \"hamming_distance\"])\n",
    "        \n",
    "        # Map each image to its group\n",
    "        image_to_group = {}\n",
    "        for group_idx, group in enumerate(duplicate_groups, 1):\n",
    "            for image in group:\n",
    "                image_to_group[image] = f\"Group_{group_idx}\"\n",
    "        \n",
    "        # Add group information to verified pairs\n",
    "        for img1, img2, dist in verified:\n",
    "            group_id = image_to_group.get(img1, \"Unknown\")\n",
    "            writer.writerow([group_id, img1, img2, dist])\n",
    "    print(f\" Detailed pairs with groups saved to: {detailed_csv}\")\n",
    "except PermissionError:\n",
    "    print(f\" Error: Cannot write to {detailed_csv}\")\n",
    "    print(\"   The file is currently open in another program.\")\n",
    "    print(\"   Please close the file and run this cell again.\")\n",
    "\n",
    "# Summary statistics\n",
    "total_images_in_dataset = len(filenames)\n",
    "images_with_duplicates = sum(len(group) for group in duplicate_groups)\n",
    "unique_images = total_images_in_dataset - images_with_duplicates + len(duplicate_groups)\n",
    "\n",
    "print(f\"\\n Dataset Summary:\")\n",
    "print(f\"   Total images in dataset: {total_images_in_dataset}\")\n",
    "print(f\"   Images with duplicates: {images_with_duplicates}\")\n",
    "print(f\"   Unique images (no duplicates): {unique_images}\")\n",
    "print(f\"   Duplicate rate: {(images_with_duplicates/total_images_in_dataset)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4103548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image1</th>\n",
       "      <th>Image2</th>\n",
       "      <th>Hamming</th>\n",
       "      <th>SSIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image845.jpg</td>\n",
       "      <td>image986.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image393.jpg</td>\n",
       "      <td>image843.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image1401.jpg</td>\n",
       "      <td>image1450.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image1299.jpg</td>\n",
       "      <td>image897.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image417.jpg</td>\n",
       "      <td>image505.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>image207.jpg</td>\n",
       "      <td>image302.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>image1296.jpg</td>\n",
       "      <td>image557.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>image1310.jpg</td>\n",
       "      <td>image876.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>image395.jpg</td>\n",
       "      <td>image988.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>image101.jpg</td>\n",
       "      <td>image216.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image1         Image2  Hamming  SSIM\n",
       "0   image845.jpg   image986.jpg        0   1.0\n",
       "1   image393.jpg   image843.jpg        0   1.0\n",
       "2  image1401.jpg  image1450.jpg        0   1.0\n",
       "3  image1299.jpg   image897.jpg        0   1.0\n",
       "4   image417.jpg   image505.jpg        0   1.0\n",
       "5   image207.jpg   image302.jpg        0   1.0\n",
       "6  image1296.jpg   image557.jpg        0   1.0\n",
       "7  image1310.jpg   image876.jpg        0   1.0\n",
       "8   image395.jpg   image988.jpg        0   1.0\n",
       "9   image101.jpg   image216.jpg        0   1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_ssim(path1, path2):\n",
    "    a = cv2.imread(str(path1), cv2.IMREAD_GRAYSCALE)\n",
    "    b = cv2.imread(str(path2), cv2.IMREAD_GRAYSCALE)\n",
    "    if a is None or b is None:\n",
    "        return None\n",
    "    h, w = min(a.shape[0], b.shape[0]), min(a.shape[1], b.shape[1])\n",
    "    a = cv2.resize(a, (w, h))\n",
    "    b = cv2.resize(b, (w, h))\n",
    "    return ssim(a, b)\n",
    "\n",
    "sample = verified[:30]\n",
    "ssim_scores = []\n",
    "for fn1, fn2, dist in sample:\n",
    "    s = compute_ssim(DATA_DIR / fn1, DATA_DIR / fn2)\n",
    "    ssim_scores.append((fn1, fn2, dist, s))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ssim_scores, columns=[\"Image1\", \"Image2\", \"Hamming\", \"SSIM\"])\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a359d7",
   "metadata": {},
   "source": [
    "### Pipeline Workflow\n",
    "\n",
    "**Step 1: Feature Extraction (Perceptual Hashing)**\n",
    "- Each image is converted into a perceptual hash (pHash) — a 64-bit compact representation\n",
    "- pHash captures the visual \"essence\" of an image while being robust to minor variations\n",
    "- Unlike cryptographic hashes, perceptual hashes produce similar values for visually similar images\n",
    "- The hash is computed from the Discrete Cosine Transform (DCT) of the grayscale image\n",
    "\n",
    "**Step 2: Set Representation**\n",
    "- Each 64-bit hash is converted into a set of bit positions where the bit value is 1\n",
    "- This set representation enables the use of Jaccard similarity and MinHash techniques\n",
    "- Example: hash `0b1010...` becomes set `{0, 2, ...}` (positions of 1-bits)\n",
    "\n",
    "**Step 3: MinHash Signature Generation**\n",
    "- Apply 100 hash functions to each bit-position set to create MinHash signatures\n",
    "- MinHash preserves Jaccard similarity: similar sets produce similar signatures\n",
    "- Reduces comparison complexity while maintaining similarity relationships\n",
    "- Each signature is a 100-element vector of minimum hash values\n",
    "\n",
    "**Step 4: Locality Sensitive Hashing (LSH)**\n",
    "- Divide each 100-element signature into 20 bands of 5 rows each\n",
    "- Hash each band independently and group images with matching band hashes into buckets\n",
    "- Images in the same bucket are candidate duplicates (probability increases with more matching bands)\n",
    "- This reduces the search space from O(n²) to approximately O(n) for finding similar pairs\n",
    "\n",
    "**Step 5: Verification**\n",
    "- Candidate pairs are verified by computing Hamming distance between original pHashes\n",
    "- Only pairs with Hamming distance ≤ 5 bits (out of 64) are confirmed as duplicates\n",
    "- This threshold allows ~7.8% difference, tolerating minor variations\n",
    "- Additionally, SSIM (Structural Similarity Index) is computed for comparison with traditional methods\n",
    "\n",
    "**Step 6: Grouping**\n",
    "- Verified duplicate pairs are grouped using Union-Find (disjoint set) algorithm\n",
    "- This identifies clusters where A≈B and B≈C implies all three are duplicates\n",
    "- Results in transitive duplicate groups rather than just pairwise matches\n",
    "\n",
    "### Comparison: MinHash/LSH vs Traditional Methods (SSIM)\n",
    "\n",
    "| Aspect | Perceptual Hash + MinHash + LSH | SSIM (Traditional) |\n",
    "|--------|----------------------------------|---------------------|\n",
    "| **Type** | Probabilistic, hash-based | Deterministic, pixel-based |\n",
    "| **Scalability** |O(n) with LSH | O(n²) comparisons needed |\n",
    "| **Speed** | Very fast (milliseconds per image) | Slow (requires full pixel comparison) |\n",
    "| **Memory** | Low (64-bit hash per image) | High (needs full images loaded) |\n",
    "| **Near-duplicate detection** | Tolerates minor changes | Very sensitive to small pixel changes |\n",
    "| **Exact duplicate detection** | Good | Good |\n",
    "| **Rotation invariance** | Not built-in (needs modification) | Not rotation-invariant |\n",
    "| **Resize/crop tolerance** | Limited tolerance | Requires same dimensions |\n",
    "| **Heavy transformations** | Misses heavily edited images | Also misses major transformations |\n",
    "| **Best use case** | Large-scale duplicate detection (millions of images) | Small-scale, high-accuracy verification |\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Strengths of MinHash + LSH:**\n",
    "- Scales to massive datasets (tested on 1000+ images here, works for millions)\n",
    "- Fast candidate generation via LSH banding (avoids all-pairs comparison)\n",
    "- Effective for finding near-duplicates with minor variations (compression artifacts, slight color adjustments, minor crops)\n",
    "- Memory efficient with compact hash representations\n",
    "- Adjustable sensitivity via `HAMMING_THRESHOLD` and LSH band configuration\n",
    "\n",
    "**Limitations:**\n",
    "- Requires careful threshold tuning (`HAMMING_THRESHOLD`, `NUM_BANDS`)\n",
    "- May miss images with significant transformations (heavy filters, rotations, major crops)\n",
    "- Probabilistic nature means some similar pairs might be missed (false negatives)\n",
    "- Can produce false positives if threshold is too high\n",
    "- Not robust to rotation without additional preprocessing\n",
    "\n",
    "**When to Use SSIM:**\n",
    "- Final verification step (as demonstrated in this notebook)\n",
    "- Small datasets where O(n²) comparisons are computationally feasible\n",
    "- Need precise pixel-level similarity scores (0.0 to 1.0 range)\n",
    "- Quality assessment for image processing pipelines\n",
    "- Ground truth validation for other methods\n",
    "\n",
    "**Performance Considerations:**\n",
    "- **LSH Tuning**: With 20 bands × 5 rows, probability of detection ≈ (1-(1-s^5)^20) where s is Jaccard similarity\n",
    "- **Threshold Selection**: `HAMMING_THRESHOLD = 5` allows ~7.8% bit differences, suitable for compressed/resized duplicates\n",
    "- **Trade-off**: More bands → higher precision but lower recall; fewer bands → higher recall but more false positives\n",
    "\n",
    "**Conclusion:**  \n",
    "The MinHash + LSH approach successfully detects duplicate and near-duplicate images efficiently at scale, making it suitable for real-world applications like photo deduplication, copyright detection, and content moderation. SSIM serves as an excellent validation metric but is impractical as a primary detection method for large datasets due to its O(n²) complexity. This hybrid approach leverages the speed of LSH for candidate generation and the accuracy of traditional metrics for verification, achieving both scalability and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
